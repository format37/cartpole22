# -*- coding: utf-8 -*-
"""rl hack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_JvMONkQgF_i0WKTH9rb6_hauqRMfaNC
"""

!pip install stable-baselines3
!pip install stable-baselines3[extra]
!pip install box2d-py
!pip install --upgrade --quiet cloudpickle pickle5
!apt-get install x11-utils > /dev/null 2>&1 
!pip install pyglet > /dev/null 2>&1 
!pip install gym[box2d] pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1
!pip install colabgymrender==1.0.2
!pip install ale-py==0.7.4 # To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)

import gym
import json

# from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
# from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.

from stable_baselines3 import PPO, A2C, TD3, SAC
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_vec_env

# from stable_baselines3 import results_plotter
# from stable_baselines3.bench import Monitor
# from stable_baselines3.results_plotter import load_results, ts2xy
# from stable_baselines3.common.callbacks import BaseCallback

# class SaveOnBestTrainingRewardCallback(BaseCallback):
#     """
#     Callback for saving a model (the check is done every ``check_freq`` steps)
#     based on the training reward (in practice, we recommend using ``EvalCallback``).

#     :param check_freq: (int)
#     :param log_dir: (str) Path to the folder where the model will be saved.
#       It must contains the file created by the ``Monitor`` wrapper.
#     :param verbose: (int)
#     """
#     def __init__(self, check_freq: int, log_dir: str, verbose=1):
#         super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)
#         self.check_freq = check_freq
#         self.log_dir = log_dir
#         self.save_path = os.path.join(log_dir, 'best_model')
#         self.best_mean_reward = -np.inf

#     def _init_callback(self) -> None:
#         # Create folder if needed
#         if self.save_path is not None:
#             os.makedirs(self.save_path, exist_ok=True)

#     def _on_step(self) -> bool:
#         if self.n_calls % self.check_freq == 0:

#           # Retrieve training reward
#           x, y = ts2xy(load_results(self.log_dir), 'timesteps')
#           if len(x) > 0:
#               # Mean training reward over the last 100 episodes
#               mean_reward = np.mean(y[-100:])
#               if self.verbose > 0:
#                 print("Num timesteps: {}".format(self.num_timesteps))
#                 print("Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}".format(self.best_mean_reward, mean_reward))

#               # New best model, you could save the agent here
#               if mean_reward > self.best_mean_reward:
#                   self.best_mean_reward = mean_reward
#                   # Example for saving best model
#                   if self.verbose > 0:
#                     print("Saving new best model to {}".format(self.save_path))
#                   self.model.save(self.save_path)

#         return True

N_ENVS = 16
ENV_NAME = 'BipedalWalker-v3'
model_name = "SAC_pretrained_nextgrid_hp_modified"
logs_file_name = model_name + '_logs.txt'
log_dir = '/tmp/'

env = gym.make(ENV_NAME)
env.reset()
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space Shape", env.observation_space.shape)
print("Sample observation", env.observation_space.sample()) # Get a random observation

print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space)
print("Action Space Sample", env.action_space.sample()) # Take a random action

# env = make_vec_env(ENV_NAME, n_envs=N_ENVS)

# model = PPO(
#     policy = 'MlpPolicy',
#     env = env,
#     n_steps = 1024,
#     batch_size = 64,
#     n_epochs = 4,
#     gamma = 0.999,
#     gae_lambda = 0.98,
#     ent_coef = 0.01,
#     verbose=1)

# eval_env = gym.make(ENV_NAME)
# mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)
# print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")

# # model = A2C('MlpPolicy', env, verbose=0)
# policy = 'MlpPolicy'
# algo = A2C
# hyperparams = {
#     'gamma':0.99,
#     'n_steps':5,
#     'vf_coef':0.25,
#     'ent_coef':0.01,
#     'max_grad_norm':0.5,
#     'learning_rate':0.0007
#     # 'alpha':0.99,
#     # 'momentum':0.0,
#     # 'epsilon':1e-05
# }
# model = algo(policy, env, verbose=1, **hyperparams)

# policy = 'MlpPolicy'
# algo = TD3
# hyperparams = {
#     'gamma':0.99,
#     'learning_rate':0.0003,
#     'buffer_size':50000,
#     'learning_starts':100,
#     'train_freq':100,
#     'gradient_steps':100,
#     'batch_size':128,
#     'tau':0.005,
#     'policy_delay':2,
#     'action_noise':None,
#     'target_policy_noise':0.2,
#     'target_noise_clip':0.5,
#     # 'random_exploration':0.0,
# }
# model = algo(policy, env, verbose=1, **hyperparams)

policy = 'MlpPolicy'
algo = SAC
## nextgrid hp
# hyperparams = {'batch_size': 128, 'buffer_size': 50000, 'gamma': 0.99, 
#               'learning_starts': 1000, 'log_std_init': 0.409723, 
#               'lr': 0.000314854, 'net_arch': 'medium', 
#               'tau': 0.02, 'ent_coef': 'auto', 'target_entropy': 'auto',
#               'train_freq': 128}
hyperparams = {'batch_size': 256, 'buffer_size': 50000, 'gamma': 0.99, 
              'learning_starts': 1000, 'log_std_init': 0.409723, 
              'lr': 0.005, 'net_arch': 'medium', 
              'tau': 0.02, 'ent_coef': 'auto', 'target_entropy': 'auto',
              'train_freq': 128}
# hyperparams = {
#     'gamma':0.99,
#     'learning_rate':0.0003,
#     'buffer_size':50000,
#     'learning_starts':100,
#     'train_freq':100,
#     'gradient_steps':100,
#     'batch_size':128,
#     'tau':0.005,
#     'action_noise':None,
# }
# model = algo(policy, env, verbose=1, **hyperparams)
model = SAC.load('SAC-Mlp', env=env, verbose=1, **hyperparams)

try:
    with open(logs_file_name, 'a') as pfile:
        pfile.write('===============\n')
        pfile.write('---PARAMS---\n')
        pfile.write(str(algo) + '\n')
        pfile.write(policy + '\n')
        for hp_name, hp_value in hyperparams.items():
            pfile.write("{} : {} \n".format(hp_name, hp_value))
    model.learn(total_timesteps=500000, log_interval=5) # , callback=callback
except KeyboardInterrupt:
    print('Training was stopped')
finally:
    model.save(model_name)
    print('Model saved')

eval_env = gym.make(ENV_NAME)
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100, deterministic=True)
print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")
with open(logs_file_name, 'a') as rfile:
    rfile.write('---RESULTS---\n')
    rfile.write('Mean reward: {} +/- {}\n'.format(mean_reward, std_reward))
    rfile.write('--------\n')

"""## Video Recording"""

from colabgymrender.recorder import Recorder

directory = './video/' + model_name
rec_env = Recorder(eval_env, directory)

obs = rec_env.reset()
done = False
while not done:
    action, _state = model.predict(obs)
    obs, reward, done, info = rec_env.step(action)
rec_env.play()

